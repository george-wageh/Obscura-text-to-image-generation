{"cells":[{"cell_type":"code","execution_count":null,"id":"wgCgbRP3htU4VfzSfHlfMRjM","metadata":{"id":"wgCgbRP3htU4VfzSfHlfMRjM","tags":[]},"outputs":[],"source":["!pip install --upgrade --no-cache-dir gdown\n","!pip install diffusers\n","!pip install datasets\n","!pip install peft\n","!pip install multilingual-clip torch"]},{"cell_type":"code","source":["import gdown\n","url = \"https://drive.google.com/uc?id=19-OXon37j72Zne8DKDQTPID-sd1VVreL\"\n","output = \"/content/code.zip\"  # Specify the output path and file name\n","\n","gdown.download(url, output, quiet=False)"],"metadata":{"id":"JYusQqrlsz8-"},"id":"JYusQqrlsz8-","execution_count":null,"outputs":[]},{"cell_type":"code","source":["import zipfile\n","import os\n","\n","zip_file_path = '/content/code.zip'\n","extract_folder_path = '/content/'\n","\n","# Check if the zip file exists\n","if os.path.exists(zip_file_path):\n","    with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n","        zip_ref.extractall(extract_folder_path)\n","else:\n","    print(\"The specified zip file does not exist.\")\n"],"metadata":{"id":"tsqcjQsbs1v9"},"id":"tsqcjQsbs1v9","execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"id":"eyeMR6PyNkls","metadata":{"id":"eyeMR6PyNkls"},"outputs":[],"source":["from image_utils import imageToTensor"]},{"cell_type":"code","execution_count":null,"id":"abyoQr2FNly7","metadata":{"id":"abyoQr2FNly7"},"outputs":[],"source":["from datasets import load_dataset\n","import torch\n","from torch.utils.data import Dataset\n","from torchvision import transforms\n","from PIL import Image\n","from torch.utils.data import DataLoader, Dataset\n","import matplotlib.pyplot as plt\n","import numpy as np"]},{"cell_type":"code","execution_count":null,"id":"gfFq-qtrNoz0","metadata":{"id":"gfFq-qtrNoz0"},"outputs":[],"source":["from models import Diffusion"]},{"cell_type":"code","execution_count":null,"id":"QjgK8ZvTNqKb","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7,"status":"ok","timestamp":1712823612885,"user":{"displayName":"","userId":""},"user_tz":-120},"id":"QjgK8ZvTNqKb","outputId":"213dd22d-ac5d-4a50-9454-c61e910f33e3"},"outputs":[{"name":"stdout","output_type":"stream","text":["Using device: cuda\n"]}],"source":["import torch\n","device = 'mps' if torch.backends.mps.is_available() else 'cuda' if torch.cuda.is_available() else 'cpu'\n","print(f'Using device: {device}')"]},{"cell_type":"code","execution_count":null,"id":"AqYpuk2ENrhU","metadata":{"id":"AqYpuk2ENrhU"},"outputs":[],"source":["d = Diffusion(device , \"DEIS\")"]},{"cell_type":"code","execution_count":null,"id":"ZOGj9EVbNupc","metadata":{"id":"ZOGj9EVbNupc"},"outputs":[],"source":["ims = d.generate([\"apple\" , \"car in sea\"] ,samples= 2, scale_guide=15, timesteps = 50 , resolution=(512,512))"]},{"cell_type":"code","execution_count":null,"id":"8jAcI9lbNwBc","metadata":{"id":"8jAcI9lbNwBc"},"outputs":[],"source":["!pip install -U git+https://github.com/openai/CLIP.git"]},{"cell_type":"code","execution_count":null,"id":"7TW9c3RTNxfb","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7505,"status":"ok","timestamp":1712823719224,"user":{"displayName":"","userId":""},"user_tz":-120},"id":"7TW9c3RTNxfb","outputId":"f676fb33-e4a6-4fc4-b68a-52aa66b6015c"},"outputs":[{"name":"stderr","output_type":"stream","text":["100%|████████████████████████████████████████| 338M/338M [00:02<00:00, 156MiB/s]\n"]}],"source":["import torch\n","import clip\n","from PIL import Image\n","model, preprocess = clip.load('ViT-B/32')\n","model = model.to(device)\n","def get_clip_score(image, text):\n","# Load the pre-trained CLIP model and the image\n","\n","    # Preprocess the image and tokenize the text\n","    image_input = preprocess(image).unsqueeze(0)\n","    text_input = clip.tokenize([text] , truncate=True)\n","\n","    # Move the inputs to GPU if available\n","    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","    image_input = image_input.to(device)\n","    text_input = text_input.to(device)\n","\n","\n","    # Generate embeddings for the image and text\n","    with torch.no_grad():\n","        image_features = model.encode_image(image_input)\n","        text_features = model.encode_text(text_input)\n","\n","    # Normalize the features\n","    image_features = image_features / image_features.norm(dim=-1, keepdim=True)\n","    text_features = text_features / text_features.norm(dim=-1, keepdim=True)\n","\n","    # Calculate the cosine similarity to get the CLIP score\n","    clip_score = torch.matmul(image_features, text_features.T).item()\n","\n","    return clip_score\n"]},{"cell_type":"code","execution_count":null,"id":"tJ9S_m84NyHM","metadata":{"id":"tJ9S_m84NyHM"},"outputs":[],"source":["evaluate_dataset = load_dataset(\"nateraw/parti-prompts\", split=\"train\")"]},{"cell_type":"code","execution_count":null,"id":"tbBTjhtGN1O0","metadata":{"id":"tbBTjhtGN1O0"},"outputs":[],"source":["arrScores = []"]},{"cell_type":"code","execution_count":null,"id":"3s5WamFfN2t8","metadata":{"id":"3s5WamFfN2t8"},"outputs":[],"source":["for i in range(int(len(evaluate_dataset)//15)):\n","  Prompts = []\n","  for j in range(15):\n","    Prompts.append(evaluate_dataset[i*15 + j][\"Prompt\"])\n","  ims,_ = d.generate(Prompts , scale_guide=11, timesteps = 50 ,dataset_ = \"None\", resolution=(512,512))\n","  for j in range(15):\n","    score = get_clip_score(ims[j], Prompts[j])\n","    sample = evaluate_dataset[i*15 + j]\n","    s = {\"Prompt\" : sample[\"Prompt\"] ,\"image\" : ims[j], \"Category\" :sample[\"Category\"] , \"Challenge\" :sample[\"Challenge\"] , \"CLIP Score\":score }\n","    arrScores.append(s)\n","    print(f\"{s}\")\n"]},{"cell_type":"code","execution_count":null,"id":"aa3c9d2b","metadata":{"id":"aa3c9d2b"},"outputs":[],"source":["categories ={}\n","challenges ={}"]},{"cell_type":"code","execution_count":null,"id":"eb1ed58e","metadata":{"id":"eb1ed58e"},"outputs":[],"source":["for i in range(len(arrScores)):\n","  if (arrScores[i][\"Category\"] not in categories):\n","    categories[arrScores[i][\"Category\"]]=0\n","  if (arrScores[i][\"Challenge\"] not in challenges):\n","    challenges[arrScores[i][\"Challenge\"]]=0"]},{"cell_type":"code","execution_count":null,"id":"26c58bb7","metadata":{"id":"26c58bb7"},"outputs":[],"source":["for key in categories:\n","  sum_ = 0\n","  len_ = 0\n","  for i in range(len(arrScores)):\n","    if (arrScores[i][\"Category\"] == key):\n","      sum_ += arrScores[i][\"CLIP Score\"]\n","      len_+=1\n","  categories[key] = sum_/len_"]},{"cell_type":"code","execution_count":null,"id":"9060c162","metadata":{"id":"9060c162"},"outputs":[],"source":["for key in challenges:\n","  sum_ = 0\n","  len_ = 0\n","  for i in range(len(arrScores)):\n","    if (arrScores[i][\"Challenge\"] == key):\n","      sum_ += arrScores[i][\"CLIP Score\"]\n","      len_+=1\n","  challenges[key] = sum_/len_"]},{"cell_type":"code","execution_count":null,"id":"76577a97","metadata":{"id":"76577a97"},"outputs":[],"source":["sum_ = 0\n","len_ = 0\n","for i in range(len(arrScores)):\n","    sum_ += arrScores[i][\"CLIP Score\"]\n","    len_+=1\n","sum_/len_"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.10"}},"nbformat":4,"nbformat_minor":5}